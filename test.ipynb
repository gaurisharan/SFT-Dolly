{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Summarize the following historical passage:', 'input': '59\\nreinforcements from England, passed new laws so\\nthat the rebels could be convicted with ease, and then\\nmoved into the storm centres of the revolt. Delhi was\\nrecaptured from the rebel forces in September 1857. The\\nlast Mughal emperor, Bahadur Shah Zafar was tried in\\ncourt and sentenced to life imprisonment. He and his\\nwife Begum Zinat Mahal were sent to prison in Rangoon\\nin October 1858. Bahadur Shah Zafar died in the Rangoon\\njail in November 1862.\\nThe recapture of Delhi, however, did not mean that the\\nrebellion died down after that. People continued to resist\\nand battle the British. The British had to fight for two\\nyears to suppress the massive forces of popular rebellion.\\nLucknow was taken in  March 1858. Rani Lakshmibai\\nwas defeated and killed in June 1858. A similar fate\\nawaited Rani Avantibai, who after initial victory in\\nKheri, chose to embrace death when surrounded by the\\nBritish on all sides. Tantia Tope escaped to the jungles\\nof central India and continued to fight a guerrilla  war\\nwith the support of many tribal and peasant leaders.\\nHe was captured, tried and killed in April 1859.\\nJust as victories against the British had earlier\\nencouraged rebellion, the defeat of rebel forces\\nencouraged desertions. The British also tried their best\\nto win back the loyalty of the people. They announced\\nrewards for loyal landholders would be allowed to\\ncontinue to enjoy traditional rights over their lands.\\nThose who had rebelled were told that if they submitted\\nto the British, and if they had not killed any white people,\\nActivity\\nMake a list of places\\nwhere the uprising took\\nplace in May, June and\\nJuly 1857.\\n\\uf086\\nWHEN PEOPLE REBEL\\nFig. 12â€“ The siege train reaches\\nDelhi\\nThe British forces initially found it\\ndifficult to break through the\\nheavy fortification in Delhi.  On 3\\nSeptember 1857 reinforcements\\narrived â€“ a 7- mile-long siege train\\ncomprising cartloads of canons\\nand ammunition pulled by\\nelephants.\\nFig. 13 â€“  Postal stamp Essued in\\ncommemoration of Tantia Tope\\n2019-200', 'output': '59 reinforcements from England, passed new laws so that the rebels could be convicted with ease, and then moved into the storm centres of the revolt. Delhi was recaptured from the rebel forces in September 1857. The last Mughal emperor, Bahadur Shah Zafar was tried in court and sentenced to...'}\n",
      "trainable params: 2,621,440 || all params: 2,777,707,520 || trainable%: 0.0944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/mukesh/.local/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_34106/3705277963.py:78: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/raid/home/mukesh/.local/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbafb1f5897404daf2972fbe0cda75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c722741ba5740c4b50befa8ec0d11b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6532741fe74a6ba13eb4522ece0ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926537f759634fe78d74776434815ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c72db1f1d2d4f398ce4b0362f34cc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082ec5c8b3d04f98af7fe8fc3c8a283d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./dolly-finetuned-2/tokenizer_config.json',\n",
       " './dolly-finetuned-2/special_tokens_map.json',\n",
       " './dolly-finetuned-2/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer  # Supervised Fine-Tuning Trainer\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SFT-Dolly-HistoryDataset\"\n",
    "finetune_tags = [\"SFT\", \"Dolly\"]\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"databricks/dolly-v2-3b\"\n",
    "dataset_name = \"gauri-sharan/history-class8-dataset\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,  # Use 8-bit quantization if needed\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Inspect the dataset structure\n",
    "print(dataset[\"train\"][0])  # Check the structure of the first example\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    # Adjust the fields based on the dataset structure\n",
    "    # For example, if the dataset has \"prompt\" and \"completion\" fields:\n",
    "    text_samples = [p + \"\\n\" + c for p, c in zip(examples[\"input\"], examples[\"output\"])]\n",
    "    return tokenizer(text_samples, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset into training and testing manually\n",
    "split_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Assign train and test datasets\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query_key_value\"]  # Correct module for Dolly-v2-3B\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dolly-finetuned-2\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"none\",  # Disable reporting to wandb\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=finetune_name,\n",
    ")\n",
    "\n",
    "# Initialize the SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"./dolly-finetuned-2\")\n",
    "tokenizer.save_pretrained(\"./dolly-finetuned-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/gauri-sharan/SFT-Dolly-HistoryDataset/commit/24b665606eb7f1e62e6e8aea6ca1d08ef07dd4a4', commit_message='End of training', commit_description='', oid='24b665606eb7f1e62e6e8aea6ca1d08ef07dd4a4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/gauri-sharan/SFT-Dolly-HistoryDataset', endpoint='https://huggingface.co', repo_type='model', repo_id='gauri-sharan/SFT-Dolly-HistoryDataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")\n",
    "\n",
    "# Push to hub\n",
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Loss =Â 2.6519\n",
      "Step 20: Loss =Â 2.5915\n",
      "Step 30: Loss =Â 2.6017\n",
      "Step 40: Loss =Â 2.5678\n",
      "Step 50: Loss =Â 2.5455\n",
      "Step 60: Loss =Â 2.6074\n",
      "Step 70: Loss =Â 2.5862\n",
      "Step 80: Loss =Â 2.5285\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = trainer.state.log_history  # Get loss logs\n",
    "for log in train_loss_history:\n",
    "    if \"loss\" in log:\n",
    "        print(f\"Step {log.get('step', 'N/A')}: Loss =Â {log['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8b800285104d95bb73ec66d72b3b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
